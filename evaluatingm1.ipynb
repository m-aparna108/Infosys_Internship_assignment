{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction  \n",
        "##  Evaluation of ASR models\n",
        "\n",
        "## Audio Analysis & Automatic Speech Recognition (ASR) Evaluation\n",
        "\n",
        "This notebook explores **Automatic Speech Recognition (ASR)** using pretrained models. It shows how raw audio is converted into text and how ASR performance can be measured.\n",
        "\n",
        "**GSL English Podcast Dataset** from **Hugging Face**, which includes short English audio clips and their transcripts is used here. Basic audio preprocessing is applied before using a pretrained **Wav2Vec2 (CTC-based)** model for speech-to-text transcription.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Goals of This Notebook\n",
        "\n",
        "- Load dataset  \n",
        "- Applied essential audio preprocessing steps:\n",
        "  - Resampling to **16 kHz**\n",
        "  - **Mono** conversion\n",
        "  - **Amplitude normalization**\n",
        "- Perform **speech-to-text transcription** using a pretrained ASR model\n",
        "- Evaluate transcription quality using:\n",
        "  - **Word Error Rate (WER)**\n",
        "  - **Character Error Rate (CER)**\n",
        "- Understand how ASR performance changes as the number of evaluation samples increases\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p0wXlSyIb1Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers torchaudio librosa --quiet"
      ],
      "metadata": {
        "id": "gJa64xtNchVp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n"
      ],
      "metadata": {
        "id": "_rqEZEmeR8zp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"vietnhat/gsl-english-podcast-dataset\")\n",
        "\n",
        "samples = dataset[\"train\"].select(range(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfOSF6uESAh6",
        "outputId": "d0e87467-b29e-46e7-aba3-316d2f03d5c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-base-960h\"\n",
        ").to(\"cuda\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkUKx70aSqVh",
        "outputId": "31ef86d0-3543-4bd4-a519-6eb392485c2e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install torchcodec --quiet"
      ],
      "metadata": {
        "id": "TIXMSao9VMRF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer --quiet\n"
      ],
      "metadata": {
        "id": "0jgc419EZNvn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jiwer import wer, cer\n",
        "\n",
        "all_references = []\n",
        "all_predictions = []\n"
      ],
      "metadata": {
        "id": "chxMhbneZVkO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_sr = 16000\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "    print(f\"\\n========== AUDIO CLIP {i+1} ==========\")\n",
        "\n",
        "    #  Load audio\n",
        "    audio = sample[\"audio\"][\"array\"]\n",
        "    sr = sample[\"audio\"][\"sampling_rate\"]\n",
        "    print(\"Original SR:\", sr, \"| Length:\", len(audio))\n",
        "\n",
        "    # Convert to mono (if stereo)\n",
        "    if audio.ndim > 1:\n",
        "        audio = np.mean(audio, axis=1)\n",
        "    print(\"Mono audio length:\", len(audio))\n",
        "\n",
        "    # Resample to 16 kHz\n",
        "    if sr != target_sr:\n",
        "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
        "    print(\"Resampled length:\", len(audio))\n",
        "\n",
        "    #  Normalize\n",
        "    audio = audio / np.max(np.abs(audio))\n",
        "    print(\"After normalization → Min:\", np.min(audio), \"Max:\", np.max(audio))\n",
        "\n",
        "    #  Feed to Wav2Vec2\n",
        "    inputs = processor(\n",
        "        audio,\n",
        "        sampling_rate=target_sr,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    input_values = inputs.input_values.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.decode(predicted_ids[0])\n",
        "\n",
        "        # Store for evaluation\n",
        "    all_references.append(sample[\"text\"].lower().strip())\n",
        "    all_predictions.append(transcription.lower().strip())\n",
        "\n",
        "\n",
        "    # Output\n",
        "    print(\"Ground Truth:\")\n",
        "    print(sample[\"text\"])\n",
        "    print(\"Prediction:\")\n",
        "    print(transcription.lower())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEMqcA_6WITC",
        "outputId": "1909d605-5315-47a9-d326-b9cf2ab21600"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== AUDIO CLIP 1 ==========\n",
            "Original SR: 16000 | Length: 143198\n",
            "Mono audio length: 143198\n",
            "Resampled length: 143198\n",
            "After normalization → Min: -1.0 Max: 0.9409418\n",
            "Ground Truth:\n",
            " Hello there everyone and welcome back to GSL English. My name is Gideon and in today's lesson\n",
            "Prediction:\n",
            "hello there every one and welcome back to geersal english my name is gideon and into day's lesson\n",
            "\n",
            "========== AUDIO CLIP 2 ==========\n",
            "Original SR: 16000 | Length: 134558\n",
            "Mono audio length: 134558\n",
            "Resampled length: 134558\n",
            "After normalization → Min: -1.0 Max: 0.9253715\n",
            "Ground Truth:\n",
            " we are going to study English together through a short story. So if you are new here let me just\n",
            "Prediction:\n",
            "we are going to study english together through a short story so if you are new here let me just\n",
            "\n",
            "========== AUDIO CLIP 3 ==========\n",
            "Original SR: 16000 | Length: 125918\n",
            "Mono audio length: 125918\n",
            "Resampled length: 125918\n",
            "After normalization → Min: -1.0 Max: 0.8111806\n",
            "Ground Truth:\n",
            " very briefly explain how this lesson is going to work. So we are firstly going to read the story\n",
            "Prediction:\n",
            "very briefly explain how this lesson is going to work so we are firstly going to read the story\n",
            "\n",
            "========== AUDIO CLIP 4 ==========\n",
            "Original SR: 16000 | Length: 106078\n",
            "Mono audio length: 106078\n",
            "Resampled length: 106078\n",
            "After normalization → Min: -0.9654156 Max: 1.0\n",
            "Ground Truth:\n",
            " in its entirety okay and then we're just going to talk about it a little bit to make sure we\n",
            "Prediction:\n",
            "y in its entirety o k and then we're just going to talk about it a little bit to make sure wev\n",
            "\n",
            "========== AUDIO CLIP 5 ==========\n",
            "Original SR: 16000 | Length: 100318\n",
            "Mono audio length: 100318\n",
            "Resampled length: 100318\n",
            "After normalization → Min: -1.0 Max: 0.9210065\n",
            "Ground Truth:\n",
            " fully understood what was going on and then we are going to break it down\n",
            "Prediction:\n",
            "fully understood what was going on and then we are going to break it down\n",
            "\n",
            "========== AUDIO CLIP 6 ==========\n",
            "Original SR: 16000 | Length: 108318\n",
            "Mono audio length: 108318\n",
            "Resampled length: 108318\n",
            "After normalization → Min: -0.6224854 Max: 1.0\n",
            "Ground Truth:\n",
            " paragraph by paragraph isolating different terms different expressions and\n",
            "Prediction:\n",
            "paragraph by paragraph isolating different terms different expressions\n",
            "\n",
            "========== AUDIO CLIP 7 ==========\n",
            "Original SR: 16000 | Length: 99678\n",
            "Mono audio length: 99678\n",
            "Resampled length: 99678\n",
            "After normalization → Min: -1.0 Max: 0.91469824\n",
            "Ground Truth:\n",
            " vocabulary so you will most certainly come away from this lesson with\n",
            "Prediction:\n",
            "and vocabulary so you will most certainly come away from this lesson with\n",
            "\n",
            "========== AUDIO CLIP 8 ==========\n",
            "Original SR: 16000 | Length: 104462\n",
            "Mono audio length: 104462\n",
            "Resampled length: 104462\n",
            "After normalization → Min: -0.65720856 Max: 1.0\n",
            "Ground Truth:\n",
            " something new but also you'll come away with an idea of how you can study\n",
            "Prediction:\n",
            "something new but also you'll come away with an idea of how you can stud\n",
            "\n",
            "========== AUDIO CLIP 9 ==========\n",
            "Original SR: 16000 | Length: 106078\n",
            "Mono audio length: 106078\n",
            "Resampled length: 106078\n",
            "After normalization → Min: -1.0 Max: 0.7770867\n",
            "Ground Truth:\n",
            " English so please before we get into it don't forget to subscribe to my channel\n",
            "Prediction:\n",
            "dy english so please before we get into it don't forget to subscribe to my channel\n",
            "\n",
            "========== AUDIO CLIP 10 ==========\n",
            "Original SR: 16000 | Length: 119838\n",
            "Mono audio length: 119838\n",
            "Resampled length: 119838\n",
            "After normalization → Min: -1.0 Max: 0.8870908\n",
            "Ground Truth:\n",
            " I post content every week to help you speak natural, fluent and confident English.\n",
            "Prediction:\n",
            "i post content every week to help you speak natural fluent and confident english\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wer_score = wer(all_references, all_predictions)\n",
        "cer_score = cer(all_references, all_predictions)\n",
        "\n",
        "print(\"\\n========== FINAL EVALUATION ==========\")\n",
        "print(f\"Total samples evaluated: {len(all_references)}\")\n",
        "print(f\"Word Error Rate (WER): {wer_score:.3f}\")\n",
        "print(f\"Character Error Rate (CER): {cer_score:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvMlDun8Ze_b",
        "outputId": "bcf1c8b4-c9f1-4de6-d9d9-79c2fff26f7e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== FINAL EVALUATION ==========\n",
            "Total samples evaluated: 10\n",
            "Word Error Rate (WER): 0.115\n",
            "Character Error Rate (CER): 0.036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Results\n",
        "\n",
        "The Wav2Vec2-base-960h model was evaluated on the first ten audio samples of the GSL English Podcast dataset. Transcription quality was measured using Word Error Rate (WER) and Character Error Rate (CER).\n",
        "\n",
        "The model achieved a WER of 11.5% and a CER of 3.6%, demonstrating good transcription accuracy on short English speech clips. Increasing the number of evaluation samples led to a more stable and slightly improved WER, indicating consistent model performance across different audio samples."
      ],
      "metadata": {
        "id": "O88skuLCbcEL"
      }
    }
  ]
}