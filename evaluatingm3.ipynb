{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of S2T medium Model\n",
        "\n",
        "This notebook evaluates the **S2T medium** automatic speech recognition (ASR) model on the **GSL English Podcast Dataset** from **Hugging Face**. The objective is to analyze how accurately the model transcribes short English speech clips.\n",
        "\n",
        "The audio samples are preprocessed and passed through the pretrained **S2T-medium** model to generate transcriptions. Model performance is measured using standard ASR evaluation metrics:\n",
        "\n",
        "- **Word Error Rate (WER)**\n",
        "- **Character Error Rate (CER)**"
      ],
      "metadata": {
        "id": "mRtzGjEMqhf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torchaudio librosa soundfile --quiet"
      ],
      "metadata": {
        "id": "s6J3gb3cqjEW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcodec --quiet"
      ],
      "metadata": {
        "id": "eGif9n7Mqle6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer --quiet"
      ],
      "metadata": {
        "id": "ITE-8LJ3qn7I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
        "from jiwer import wer, cer"
      ],
      "metadata": {
        "id": "KzKwZtA6qqpH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"vietnhat/gsl-english-podcast-dataset\")\n",
        "samples = dataset[\"train\"].select(range(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZoQ9Ul5qwII",
        "outputId": "757139c1-e7f4-48ff-9c00-1105555961cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-medium-librispeech-asr\")\n",
        "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-medium-librispeech-asr\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "W7dOQhIPq42H",
        "outputId": "104c03d1-8281-4d41-950a-4aae580fa9cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Speech2TextForConditionalGeneration(\n",
              "  (model): Speech2TextModel(\n",
              "    (encoder): Speech2TextEncoder(\n",
              "      (conv): Conv1dSubsampler(\n",
              "        (conv_layers): ModuleList(\n",
              "          (0): Conv1d(80, 1024, kernel_size=(5,), stride=(2,), padding=(2,))\n",
              "          (1): Conv1d(512, 1024, kernel_size=(5,), stride=(2,), padding=(2,))\n",
              "        )\n",
              "      )\n",
              "      (embed_positions): Speech2TextSinusoidalPositionalEmbedding()\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x Speech2TextEncoderLayer(\n",
              "          (self_attn): Speech2TextAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): ReLU()\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): Speech2TextDecoder(\n",
              "      (embed_tokens): Embedding(10000, 512, padding_idx=1)\n",
              "      (embed_positions): Speech2TextSinusoidalPositionalEmbedding()\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x Speech2TextDecoderLayer(\n",
              "          (self_attn): Speech2TextAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): Speech2TextAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=10000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_sr = 16000\n",
        "predictions = []\n",
        "ground_truths = []\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "    print(f\"\\n========== AUDIO CLIP {i+1} ==========\")\n",
        "\n",
        "    # Load audio\n",
        "    audio = sample[\"audio\"][\"array\"]\n",
        "    sr = sample[\"audio\"][\"sampling_rate\"]\n",
        "    print(\"Original SR:\", sr, \"| Length:\", len(audio))\n",
        "\n",
        "    # Convert to mono\n",
        "    if audio.ndim > 1:\n",
        "        audio = np.mean(audio, axis=1)\n",
        "    print(\"Mono audio length:\", len(audio))\n",
        "\n",
        "    # Resample to 16 kHz\n",
        "    if sr != target_sr:\n",
        "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
        "    print(\"Resampled length:\", len(audio))\n",
        "\n",
        "    # Normalize\n",
        "    audio = audio / np.max(np.abs(audio))\n",
        "    print(\"After normalization → Min:\", np.min(audio), \"Max:\", np.max(audio))\n",
        "\n",
        "    inputs = processor(audio, sampling_rate=target_sr, return_tensors=\"pt\").input_features.to(device)\n",
        "\n",
        "    generated_ids = model.generate(inputs)\n",
        "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    predictions.append(transcription.lower())\n",
        "    ground_truths.append(sample[\"text\"].lower())\n",
        "\n",
        "    print(\"Ground Truth:\", sample[\"text\"])\n",
        "    print(\"Prediction  :\", transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34TTBU0rrFuZ",
        "outputId": "227583b5-9a7d-4c45-f142-0b8d8b80250d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== AUDIO CLIP 1 ==========\n",
            "Original SR: 16000 | Length: 143198\n",
            "Mono audio length: 143198\n",
            "Resampled length: 143198\n",
            "After normalization → Min: -1.0 Max: 0.9409418\n",
            "Ground Truth:  Hello there everyone and welcome back to GSL English. My name is Gideon and in today's lesson\n",
            "Prediction  : hallo there everyone and welcome back to g s o english my name is gideon and into days lesser\n",
            "\n",
            "========== AUDIO CLIP 2 ==========\n",
            "Original SR: 16000 | Length: 134558\n",
            "Mono audio length: 134558\n",
            "Resampled length: 134558\n",
            "After normalization → Min: -1.0 Max: 0.9253715\n",
            "Ground Truth:  we are going to study English together through a short story. So if you are new here let me just\n",
            "Prediction  : we are going to study english together through a short story so if you are new here let me just\n",
            "\n",
            "========== AUDIO CLIP 3 ==========\n",
            "Original SR: 16000 | Length: 125918\n",
            "Mono audio length: 125918\n",
            "Resampled length: 125918\n",
            "After normalization → Min: -1.0 Max: 0.8111806\n",
            "Ground Truth:  very briefly explain how this lesson is going to work. So we are firstly going to read the story\n",
            "Prediction  : very briefly explain how this lesson is going to work so we are firstly going to read the story\n",
            "\n",
            "========== AUDIO CLIP 4 ==========\n",
            "Original SR: 16000 | Length: 106078\n",
            "Mono audio length: 106078\n",
            "Resampled length: 106078\n",
            "After normalization → Min: -0.9654156 Max: 1.0\n",
            "Ground Truth:  in its entirety okay and then we're just going to talk about it a little bit to make sure we\n",
            "Prediction  : in its entirety o k and then we're just going to talk about it a little bit to make sure we're sure we\n",
            "\n",
            "========== AUDIO CLIP 5 ==========\n",
            "Original SR: 16000 | Length: 100318\n",
            "Mono audio length: 100318\n",
            "Resampled length: 100318\n",
            "After normalization → Min: -1.0 Max: 0.9210065\n",
            "Ground Truth:  fully understood what was going on and then we are going to break it down\n",
            "Prediction  : fully understood what was going on and then we are going to break it down\n",
            "\n",
            "========== AUDIO CLIP 6 ==========\n",
            "Original SR: 16000 | Length: 108318\n",
            "Mono audio length: 108318\n",
            "Resampled length: 108318\n",
            "After normalization → Min: -0.6224854 Max: 1.0\n",
            "Ground Truth:  paragraph by paragraph isolating different terms different expressions and\n",
            "Prediction  : paragraph by paragraph isolating different terms different expressions\n",
            "\n",
            "========== AUDIO CLIP 7 ==========\n",
            "Original SR: 16000 | Length: 99678\n",
            "Mono audio length: 99678\n",
            "Resampled length: 99678\n",
            "After normalization → Min: -1.0 Max: 0.91469824\n",
            "Ground Truth:  vocabulary so you will most certainly come away from this lesson with\n",
            "Prediction  : and vocabulary so you will most certainly come away from this lesson we\n",
            "\n",
            "========== AUDIO CLIP 8 ==========\n",
            "Original SR: 16000 | Length: 104462\n",
            "Mono audio length: 104462\n",
            "Resampled length: 104462\n",
            "After normalization → Min: -0.65720856 Max: 1.0\n",
            "Ground Truth:  something new but also you'll come away with an idea of how you can study\n",
            "Prediction  : something new but also you'll come away with an idea of how you can stud\n",
            "\n",
            "========== AUDIO CLIP 9 ==========\n",
            "Original SR: 16000 | Length: 106078\n",
            "Mono audio length: 106078\n",
            "Resampled length: 106078\n",
            "After normalization → Min: -1.0 Max: 0.7770867\n",
            "Ground Truth:  English so please before we get into it don't forget to subscribe to my channel\n",
            "Prediction  : d english so please before we get into it don't forget to subscribe to my channel\n",
            "\n",
            "========== AUDIO CLIP 10 ==========\n",
            "Original SR: 16000 | Length: 119838\n",
            "Mono audio length: 119838\n",
            "Resampled length: 119838\n",
            "After normalization → Min: -1.0 Max: 0.8870908\n",
            "Ground Truth:  I post content every week to help you speak natural, fluent and confident English.\n",
            "Prediction  : i post content every week to help you speak natural fluent and confident english english\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_wer = wer(ground_truths, predictions)\n",
        "final_cer = cer(ground_truths, predictions)\n",
        "\n",
        "print(\"\\n========== FINAL EVALUATION ==========\")\n",
        "print(\"Total samples evaluated:\", len(samples))\n",
        "print(f\"Word Error Rate (WER): {final_wer:.3f}\")\n",
        "print(f\"Character Error Rate (CER): {final_cer:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6Nkdy4WrGPh",
        "outputId": "04e34ea0-55ab-4ccc-de97-7eafc2d8e5f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== FINAL EVALUATION ==========\n",
            "Total samples evaluated: 10\n",
            "Word Error Rate (WER): 0.141\n",
            "Character Error Rate (CER): 0.059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Summary – S2T Medium\n",
        "\n",
        "The s2t-medium-librispeech-asr model was evaluated on ten English podcast audio clips. The model achieved a Word Error Rate (WER) of 14.1% and a Character Error Rate (CER) of 5.9%, indicating moderate transcription accuracy.\n",
        "\n",
        "As this model was primarily trained on read speech from Librispeech, its performance on conversational podcast-style speech is lower compared to models like Whisper-medium."
      ],
      "metadata": {
        "id": "wncF2SIBr8fy"
      }
    }
  ]
}